from itertools import islice
import datetime
import numpy as np
import pandas as pd
import xgboost

import tensorflow as tf
import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.optimizers import SGD
import keras.backend as K
from keras.utils.np_utils import to_categorical

from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
from sklearn.utils import resample
from sklearn.ensemble import VotingClassifier

RANDOM_STATE_KFOLD = 1168
RANDOM_STATE_MODEL = 9999

def auc(y_true, y_pred):   
    ptas = tf.stack([binary_PTA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)
    pfas = tf.stack([binary_PFA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)
    pfas = tf.concat([tf.ones((1,)) ,pfas],axis=0)
    binSizes = -(pfas[1:]-pfas[:-1])
    s = ptas*binSizes
    return K.sum(s, axis=0)


# PFA, prob false alert for binary classifier
def binary_PFA(y_true, y_pred, threshold=K.variable(value=0.5)):
    y_pred = K.cast(y_pred >= threshold, 'float32')
    # N = total number of negative labels
    N = K.sum(1 - y_true)
    # FP = total number of false alerts, alerts from the negative class labels
    FP = K.sum(y_pred - y_pred * y_true)    
    return FP/N


# P_TA prob true alerts for binary classifier
def binary_PTA(y_true, y_pred, threshold=K.variable(value=0.5)):
    y_pred = K.cast(y_pred >= threshold, 'float32')
    # P = total number of positive labels
    P = K.sum(y_true)
    # TP = total number of correct alerts, alerts from the positive class labels
    TP = K.sum(y_pred * y_true)    
    return TP/P

def read_data(dataset):
    if dataset == 'train':
        file_name = './data/train_features.tsv'
        file = pd.read_csv(file_name, sep='\t')
        X = file.iloc[:,1:257].values
        y = file.iloc[:,257].values
        return(X, y)
    elif dataset == 'test':
        file_name = './data/test_features.tsv'
        file = pd.read_csv(file_name, sep='\t')
        X = file.iloc[:,1:257].values
        names = file.iloc[:,0].values
        return (np.array(X), names)

def one_hot(X):
    X[X > 0] = 1
    return(X)
    
def preprocess_features(X=None, appearance_scaler=None, train=True):
    X_processed = np.copy(X)
    if train:
        appearance_scaler = StandardScaler().fit(X_processed[:,0:3])
    X_processed[:,0:3] = appearance_scaler.transform(X_processed[:,0:3])
    X_processed[:,12:104] = one_hot(X_processed[:,12:104])
    X_processed[:,104:132] = one_hot(X_processed[:,104:132])
    X_processed[:,132:156] = one_hot(X_processed[:,132:156])
    X_processed[:,156:256] = one_hot(X_processed[:,156:256])

    return (X_processed, appearance_scaler)    

def remove_column(X, log=False, product=False, time=False, prob=False):
    index = list()
    if log:
        index.extend(list(range(12, 104)))
    if product:
        index.extend(list(range(104, 132)))
    if time:
        index.extend(list(range(132, 156)))
    if prob:
        index.extend(list(range(3, 11)))
        
    X = np.delete(X, index, axis=1)
    return(X)

def train_model(method='rf'):
    
    X_train, y_train = read_data('train')
    X_test, _ = read_data('test')
    _, appearance_scaler = preprocess_features(np.concatenate((X_train, X_test), axis=0))
    X_train, appearance_scaler = preprocess_features(X_train, appearance_scaler, train=False)
    
    if method == 'rf':
        clf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE_MODEL)
        params = {'criterion':['gini', 'entropy'], 
                  'max_depth':[2, 4, 6, 8, 10, None],
                  'min_samples_split': [0.01, 0.05, 0.1],
                  'min_samples_leaf': [0.01, 0.05, 0.1]}
    elif method == 'lr':
        clf = LogisticRegression(solver='liblinear', random_state=RANDOM_STATE_MODEL)
        params = {'penalty':['l1', 'l2'], 
                  'C':[1, 2, 4, 6, 8, 10],
                  'fit_intercept':[True, False]}
    elif method == 'xgb':
        clf = xgboost.XGBClassifier(n_estimators=100, random_state=RANDOM_STATE_MODEL)
        params = {'max_depth':[2, 4, 6, 8, 10],
                  'learning_rate':[1, 0.5, 0.1],
                  'booster':['gbtree', 'gblinear', 'dart'],
                  'max_delta_step':[0, 1, 2, 3]}   
    elif method == 'gnb':
        clf = GaussianNB()
        params = {}
        X_train = remove_column(X_train, True, True, True)
    elif method == 'ensemble':
        rf_clf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE_MODEL)
        xgb_clf = xgboost.XGBClassifier(n_estimators=100, random_state=RANDOM_STATE_MODEL)
        lr_clf = LogisticRegression(solver='liblinear', random_state=RANDOM_STATE_MODEL)
        clf = VotingClassifier(estimators=[('rf', rf_clf), ('xgb', xgb_clf), ('lr', lr_clf)], voting='soft')
        params = {'rf__criterion':['gini', 'entropy'], 
                  'rf__max_depth':[5, 10],
                  'rf__min_samples_split': [0.1],
                  'rf__min_samples_leaf': [0.1],
                  'xgb__max_depth':[5, 10],
                  'xgb__learning_rate':[1, 0.5],
                  'xgb__booster':['gbtree', 'gblinear', 'dart'],
                  'xgb__max_delta_step':[0, 2, 4],
                  'lr__penalty':['l1', 'l2'], 
                  'lr__C':[1, 5],
                  'lr__fit_intercept':[True, False]}
        
    grid = GridSearchCV(clf, params, n_jobs=-1, cv=5, verbose=1, scoring='roc_auc')
    grid.fit(X_train, y_train)
    print('Best cross validation score:', grid.best_score_)
    
    deprecated = """for i_train, i_test in skf.split(X_train, y_train):
        X_train_scale, appearance_scaler, log_scaler, product_scaler, time_scaler = preprocess_features(X_train[i_train])
        
        zero = np.where(y_train[i_train] == 0)[0]
        one = np.where(y_train[i_train] == 1)[0]
        zero = resample(zero, replace=False, n_samples=one.shape[0], random_state=RANDOM_STATE_MODEL)
        i_train = np.concatenate((one, zero))
        X_train_scale = X_train[i_train]
        
        if method == 'gnb':
            X_train_scale = remove_column(X_train_scale, True, True, True)
        
        if method == 'ann':
            clf = Sequential()
            clf.add(Dense(64, activation='relu', input_dim=X_train_scale.shape[1]))
            clf.add(Dropout(0.5))
            clf.add(Dense(64, activation='relu'))
            clf.add(Dropout(0.5))
            clf.add(Dense(2, activation='softmax'))
            sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
            clf.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy', auc])
            clf.fit(X_train_scale, to_categorical(y_train[i_train], nb_classes=2), nb_epoch=1, batch_size=128, validation_split=0.25)
        else:
            clf.fit(X_train_scale, y_train[i_train])
            
        X_test_scale, _, _, _, _ = preprocess_features(X_train[i_test], appearance_scaler, log_scaler, product_scaler, time_scaler, train=False)
        
        if method == 'gnb':
            X_test_scale = remove_column(X_test_scale, True, True, True)  
        
        if method == 'ann':
            scores.append(roc_auc_score(y_train[i_test], clf.predict_proba(X_test_scale)[:,1]))
        else:
            ind = np.where(clf.classes_==1)[0][0]
            scores.append(roc_auc_score(y_train[i_test], clf.predict_proba(X_test_scale)[:,ind]))
    
    print('Number of samples in training set:', len(X_train))
    for i, s in enumerate(scores):
        print('CV', i, ': ', s, sep='')
    print('Average cross validation scroe: ', sum(scores)/len(scores))"""

    #if method == 'ann':
    #    clf.fit(X_train, to_categorical(y_train, nb_classes=2), nb_epoch=10, batch_size=128, validation_split=0.25)
    #else:
    #    clf.fit(X_train, y_train)
    
    return appearance_scaler, grid


def predict_test_data(appearance_scaler, clf, method):
    X, names = read_data('test')
    ind = np.where(clf.classes_==1)[0][0]
    X, _, = preprocess_features(X, appearance_scaler, train=False)
    
    if method == 'gnb':
        X = remove_column(X, True, True, True)
    
    y_predict = clf.predict_proba(X)[:,ind]
    
    with open('./data/submit_{}.csv'.format(datetime.datetime.now().strftime("%Y_%m_%d_%H_%M_%S")), 'w') as f:
        for name, y in zip(names, y_predict):
            f.write('{},{}\n'.format(name, y))
            
if __name__ == '__main__':
#if True:
    methods = ['gnb', 'rf', 'xgb', 'lr', 'ensemble']
    appearance_scaler, clf = train_model('ensemble')
        
    #if method == 'rf' or method == 'xgb':
    #    print('Feature importance of {}'.format(method))
    #    print(clf.feature_importances_)
    
    predict_test_data(appearance_scaler, clf, 'ensemble')